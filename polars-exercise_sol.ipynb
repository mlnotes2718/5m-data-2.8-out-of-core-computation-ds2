{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-World Polars Data Analysis Exercise\n",
    "\n",
    "## Overview\n",
    "Welcome to this comprehensive Polars data analysis exercise! You'll work as a data analyst for **Global Analytics Inc.**, a consulting firm that helps organizations make data-driven decisions. Your task is to analyze multiple real-world datasets to provide insights for different clients.\n",
    "\n",
    "**Note**: This exercise uses sample datasets that mirror the structure of real-world data from sources like Our World in Data (COVID-19), Yahoo Finance (stocks), retail transaction data, and World Bank (population data).\n",
    "\n",
    "## Business Scenarios\n",
    "\n",
    "### Scenario 1: Public Health Analysis for World Health Organization\n",
    "**Client**: World Health Organization (WHO)  \n",
    "**Dataset**: COVID-19 global data  \n",
    "**Business Question**: Analyze global COVID-19 pandemic trends to inform public health policy decisions.\n",
    "\n",
    "### Scenario 2: Investment Analysis for PrimeTech Capital\n",
    "**Client**: PrimeTech Capital (Investment Firm)  \n",
    "**Dataset**: Stock market data for major tech companies  \n",
    "**Business Question**: Evaluate technology stock performance to guide investment strategies.\n",
    "\n",
    "### Scenario 3: Retail Strategy for SuperMart Chain\n",
    "**Client**: SuperMart (Retail Chain)  \n",
    "**Dataset**: Sales transaction data  \n",
    "**Business Question**: Optimize product mix and identify growth opportunities.\n",
    "\n",
    "### Scenario 4: Market Research for GlobalTech Corp\n",
    "**Client**: GlobalTech Corp (Technology Company)  \n",
    "**Dataset**: Global population and demographic data  \n",
    "**Business Question**: Identify target markets for expansion based on population trends.\n",
    "\n",
    "---\n",
    "\n",
    "## Getting Started\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Load all datasets\n",
    "covid_df = pl.read_csv(\"../tutorial/covid_global_data.csv\")\n",
    "stocks_df = pl.read_csv(\"../tutorial/tech_stocks_data.csv\") \n",
    "sales_df = pl.read_csv(\"../tutorial/supermart_sales_data.csv\")\n",
    "population_df = pl.read_csv(\"../tutorial/global_population_data.csv\")\n",
    "company_df = pl.read_csv(\"../tutorial/company_info_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 1: Data Exploration and Basic Operations\n",
    "\n",
    "### Exercise 1.1: Creating and Inspecting DataFrames\n",
    "\n",
    "**Business Context**: Before analyzing any dataset, you need to understand its structure and content.\n",
    "\n",
    "**Tasks**:\n",
    "1. Display the shape and basic info for each dataset\n",
    "2. Show the first 5 rows of each dataset\n",
    "3. Display column names and data types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use .shape, .head(), .columns, .dtypes\n",
    "# Display shape and basic info\n",
    "print(\"COVID Data Shape:\", covid_df.shape)\n",
    "print(\"Stocks Data Shape:\", stocks_df.shape)\n",
    "print(\"Sales Data Shape:\", sales_df.shape)\n",
    "print(\"Population Data Shape:\", population_df.shape)\n",
    "print(\"Company Data Shape:\", company_df.shape)\n",
    "\n",
    "# Show first 5 rows\n",
    "print(\"COVID Data Head:\")\n",
    "print(covid_df.head())\n",
    "\n",
    "print(\"\\nStocks Data Head:\")\n",
    "print(stocks_df.head())\n",
    "\n",
    "# Display column names and data types\n",
    "print(\"\\nCOVID Data Schema:\")\n",
    "print(covid_df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Functions**: Basic DataFrame inspection\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 1.2: Selecting Columns and Rows\n",
    "\n",
    "**Business Context**: You need to focus on specific data points for your analysis.\n",
    "\n",
    "**Tasks**:\n",
    "1. From the COVID dataset, select only 'date', 'country', and 'new_cases' columns\n",
    "2. From the stocks dataset, select columns whose names contain \"e\" (like 'date', 'close')\n",
    "3. Select the first 100 rows from the sales dataset\n",
    "4. Select rows 50-100 from the population dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use .select(), pl.col(), .head(), .slice()\n",
    "# Select specific columns from COVID data\n",
    "covid_subset = covid_df.select([\"date\", \"country\", \"new_cases\"])\n",
    "\n",
    "# Select columns containing \"e\" from stocks\n",
    "stocks_e_cols = stocks_df.select(pl.col(\"^.*e.*$\"))\n",
    "\n",
    "# First 100 rows from sales\n",
    "sales_100 = sales_df.head(100)\n",
    "\n",
    "# Rows 50-100 from population\n",
    "population_slice = population_df.slice(50, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Functions**: Column selection, row subsetting\n",
    "\n",
    "---\n",
    "\n",
    "## PART 2: Filtering and Subsetting Data\n",
    "\n",
    "### Exercise 2.1: Basic Filtering\n",
    "\n",
    "**Business Context**: WHO wants to focus on countries with significant COVID-19 impact.\n",
    "\n",
    "**Tasks**:\n",
    "1. Filter COVID data for countries with more than 1000 new cases on any single day\n",
    "2. Filter stock data for Apple (AAPL) only\n",
    "3. Filter sales data for Technology category products\n",
    "4. Filter population data for countries with more than 100 million people\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use .filter(), pl.col()\n",
    "\n",
    "# Countries with >1000 cases in a single day\n",
    "high_cases = covid_df.filter(pl.col(\"new_cases\") > 1000)\n",
    "\n",
    "# Apple stock data only\n",
    "aapl_data = stocks_df.filter(pl.col(\"symbol\") == \"AAPL\")\n",
    "\n",
    "# Technology category sales\n",
    "tech_sales = sales_df.filter(pl.col(\"category\") == \"Technology\")\n",
    "\n",
    "# Countries with >100M population\n",
    "large_countries = population_df.filter(pl.col(\"population\") > 100_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Functions**: Filtering with conditions\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.2: Complex Filtering\n",
    "\n",
    "**Business Context**: PrimeTech Capital wants to identify high-volatility tech stocks.\n",
    "\n",
    "**Tasks**:\n",
    "1. Filter stock data where the daily price range (high - low) is greater than $10\n",
    "2. Filter COVID data for dates between \"2020-03-01\" and \"2020-06-01\"\n",
    "3. Filter sales data for orders with discount > 0.15 AND profit > 100\n",
    "4. Sample 500 random rows from the sales dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use .filter() with multiple conditions, .sample()\n",
    "\n",
    "# High volatility stocks (range > $10)\n",
    "volatile_stocks = stocks_df.filter(\n",
    "    (pl.col(\"high\") - pl.col(\"low\")) > 10\n",
    ")\n",
    "\n",
    "# COVID data for specific date range\n",
    "covid_q1_2020 = covid_df.filter(\n",
    "    (pl.col(\"date\") >= \"2020-03-01\") & \n",
    "    (pl.col(\"date\") <= \"2020-06-01\")\n",
    ")\n",
    "\n",
    "# High discount AND high profit sales\n",
    "profitable_discounted = sales_df.filter(\n",
    "    (pl.col(\"discount\") > 0.15) & \n",
    "    (pl.col(\"profit\") > 100)\n",
    ")\n",
    "\n",
    "# Random sample of 500 sales records\n",
    "sales_sample = sales_df.sample(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Functions**: Complex filtering, sampling\n",
    "\n",
    "---\n",
    "\n",
    "## PART 3: Grouping and Aggregation\n",
    "\n",
    "### Exercise 3.1: Basic Grouping\n",
    "\n",
    "**Business Context**: SuperMart wants to understand sales performance by different dimensions.\n",
    "\n",
    "**Tasks**:\n",
    "1. Group sales data by 'region' and calculate total sales\n",
    "2. Group COVID data by 'country' and find maximum daily cases\n",
    "3. Group stock data by 'symbol' and calculate average closing price\n",
    "4. Count the number of records in each population dataset by year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use .group_by(), .agg(), pl.sum(), pl.max(), pl.mean(), pl.count()\n",
    "\n",
    "# Sales by region\n",
    "regional_sales = sales_df.group_by(\"region\").agg(\n",
    "    pl.col(\"sales\").sum().alias(\"total_sales\")\n",
    ")\n",
    "\n",
    "# Max daily cases by country\n",
    "max_cases_by_country = covid_df.group_by(\"country\").agg(\n",
    "    pl.col(\"new_cases\").max().alias(\"max_daily_cases\")\n",
    ")\n",
    "\n",
    "# Average closing price by stock\n",
    "avg_prices = stocks_df.group_by(\"symbol\").agg(\n",
    "    pl.col(\"close\").mean().alias(\"avg_close_price\")\n",
    ")\n",
    "\n",
    "# Record count by year\n",
    "records_by_year = population_df.group_by(\"year\").agg(\n",
    "    pl.count().alias(\"record_count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Functions**: Basic grouping and aggregation\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.2: Advanced Aggregations\n",
    "\n",
    "**Business Context**: WHO needs comprehensive statistics for their pandemic response.\n",
    "\n",
    "**Tasks**:\n",
    "1. For each country in COVID data, calculate:\n",
    "   - Total cases, maximum daily cases, average daily cases\n",
    "   - Standard deviation of daily cases, median daily cases\n",
    "2. For each stock symbol, calculate:\n",
    "   - Mean closing price, min/max prices, price volatility (std dev)\n",
    "3. For sales data by category, calculate:\n",
    "   - Total sales, total profit, average discount, profit margin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use .agg() with multiple aggregation functions\n",
    "\n",
    "# Comprehensive COVID statistics by country\n",
    "covid_stats = covid_df.group_by(\"country\").agg([\n",
    "    pl.col(\"new_cases\").sum().alias(\"total_cases\"),\n",
    "    pl.col(\"new_cases\").max().alias(\"max_daily_cases\"),\n",
    "    pl.col(\"new_cases\").mean().alias(\"avg_daily_cases\"),\n",
    "    pl.col(\"new_cases\").std().alias(\"cases_std_dev\"),\n",
    "    pl.col(\"new_cases\").median().alias(\"median_daily_cases\")\n",
    "])\n",
    "\n",
    "# Stock volatility analysis\n",
    "stock_analysis = stocks_df.group_by(\"symbol\").agg([\n",
    "    pl.col(\"close\").mean().alias(\"mean_price\"),\n",
    "    pl.col(\"close\").min().alias(\"min_price\"),\n",
    "    pl.col(\"close\").max().alias(\"max_price\"),\n",
    "    pl.col(\"close\").std().alias(\"price_volatility\")\n",
    "])\n",
    "\n",
    "# Sales performance by category\n",
    "category_performance = sales_df.group_by(\"category\").agg([\n",
    "    pl.col(\"sales\").sum().alias(\"total_sales\"),\n",
    "    pl.col(\"profit\").sum().alias(\"total_profit\"),\n",
    "    pl.col(\"discount\").mean().alias(\"avg_discount\"),\n",
    "    (pl.col(\"profit\").sum() / pl.col(\"sales\").sum()).alias(\"profit_margin\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Functions**: Multiple aggregations, statistical functions\n",
    "\n",
    "---\n",
    "\n",
    "## PART 4: Time Series Analysis\n",
    "\n",
    "### Exercise 4.1: Date Operations\n",
    "\n",
    "**Business Context**: Analyzing trends over time requires proper date handling.\n",
    "\n",
    "**Tasks**:\n",
    "1. Convert date columns to datetime format in all relevant datasets\n",
    "2. Extract year, month, and day of week from COVID data dates\n",
    "3. Filter stock data for trading days in 2022 only\n",
    "4. Calculate the number of days between order_date and ship_date in sales data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use pl.col().str.strptime(), .dt.year(), .dt.month(), .dt.weekday()\n",
    "\n",
    "# Convert date columns to datetime\n",
    "covid_df = covid_df.with_columns([\n",
    "    pl.col(\"date\").str.strptime(pl.Date, \"%Y-%m-%d\").alias(\"date\")\n",
    "])\n",
    "\n",
    "stocks_df = stocks_df.with_columns([\n",
    "    pl.col(\"date\").str.strptime(pl.Date, \"%Y-%m-%d\").alias(\"date\")\n",
    "])\n",
    "\n",
    "# Extract date components\n",
    "covid_df = covid_df.with_columns([\n",
    "    pl.col(\"date\").dt.year().alias(\"year\"),\n",
    "    pl.col(\"date\").dt.month().alias(\"month\"),\n",
    "    pl.col(\"date\").dt.weekday().alias(\"day_of_week\")\n",
    "])\n",
    "\n",
    "# Filter for 2022 trading days\n",
    "stocks_2022 = stocks_df.filter(pl.col(\"date\").dt.year() == 2022)\n",
    "\n",
    "# Calculate days between order and ship dates\n",
    "sales_df = sales_df.with_columns([\n",
    "    pl.col(\"order_date\").str.strptime(pl.Date, \"%Y-%m-%d\").alias(\"order_date\"),\n",
    "    pl.col(\"ship_date\").str.strptime(pl.Date, \"%Y-%m-%d\").alias(\"ship_date\")\n",
    "]).with_columns([\n",
    "    (pl.col(\"ship_date\") - pl.col(\"order_date\")).dt.days().alias(\"days_to_ship\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Functions**: Date parsing and manipulation\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.2: Rolling Functions\n",
    "\n",
    "**Business Context**: PrimeTech Capital wants to smooth out stock price volatility with moving averages.\n",
    "\n",
    "**Tasks**:\n",
    "1. Calculate 7-day rolling average of new COVID cases for each country\n",
    "2. Calculate 30-day rolling average of stock closing prices for each symbol\n",
    "3. Calculate 7-day rolling maximum and minimum for stock prices\n",
    "4. Calculate 14-day rolling sum of sales by region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use .rolling_mean(), .rolling_max(), .rolling_min(), .rolling_sum()\n",
    "\n",
    "# 7-day rolling average for COVID cases\n",
    "covid_rolling = covid_df.sort([\"country\", \"date\"]).with_columns([\n",
    "    pl.col(\"new_cases\").rolling_mean(window_size=7).over(\"country\").alias(\"cases_7d_avg\")\n",
    "])\n",
    "\n",
    "# 30-day rolling average for stock prices\n",
    "stocks_rolling = stocks_df.sort([\"symbol\", \"date\"]).with_columns([\n",
    "    pl.col(\"close\").rolling_mean(window_size=30).over(\"symbol\").alias(\"price_30d_avg\")\n",
    "])\n",
    "\n",
    "# Rolling max and min for stocks\n",
    "stocks_rolling = stocks_rolling.with_columns([\n",
    "    pl.col(\"close\").rolling_max(window_size=7).over(\"symbol\").alias(\"price_7d_max\"),\n",
    "    pl.col(\"close\").rolling_min(window_size=7).over(\"symbol\").alias(\"price_7d_min\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Functions**: Rolling calculations\n",
    "\n",
    "---\n",
    "\n",
    "## PART 5: Window Functions\n",
    "\n",
    "### Exercise 5.1: Window Calculations\n",
    "\n",
    "**Business Context**: Compare performance within groups without losing individual records.\n",
    "\n",
    "**Tasks**:\n",
    "1. For each country, calculate the percentage of total global cases\n",
    "2. Rank stock symbols by their average closing price\n",
    "3. Calculate each region's share of total sales\n",
    "4. Find each customer's total spending and rank them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use .over(), .rank(), window functions\n",
    "\n",
    "# Percentage of total global cases by country\n",
    "covid_pct = covid_df.with_columns([\n",
    "    (pl.col(\"new_cases\") / pl.col(\"new_cases\").sum().over(\"date\") * 100)\n",
    "    .alias(\"pct_of_global_cases\")\n",
    "])\n",
    "\n",
    "# Rank stocks by average price\n",
    "stock_rankings = stocks_df.group_by(\"symbol\").agg([\n",
    "    pl.col(\"close\").mean().alias(\"avg_price\")\n",
    "]).with_columns([\n",
    "    pl.col(\"avg_price\").rank(descending=True).alias(\"price_rank\")\n",
    "])\n",
    "\n",
    "# Regional sales share\n",
    "regional_share = sales_df.with_columns([\n",
    "    (pl.col(\"sales\") / pl.col(\"sales\").sum() * 100).alias(\"pct_of_total_sales\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Functions**: Window functions, ranking\n",
    "\n",
    "---\n",
    "\n",
    "## PART 6: Handling Missing Data\n",
    "\n",
    "### Exercise 6.1: Missing Data Detection and Treatment\n",
    "\n",
    "**Business Context**: Real-world data often has missing values that need handling.\n",
    "\n",
    "**Tasks**:\n",
    "1. Find all missing values in each dataset\n",
    "2. Drop rows with any missing values from COVID data\n",
    "3. Fill missing population values with the forward fill strategy\n",
    "4. Replace missing values with the column mean for numeric columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use .drop_nulls(), .fill_null(), .is_null()\n",
    "\n",
    "# Find missing values\n",
    "covid_nulls = covid_df.null_count()\n",
    "population_nulls = population_df.null_count()\n",
    "\n",
    "# Drop rows with missing values\n",
    "covid_clean = covid_df.drop_nulls()\n",
    "\n",
    "# Forward fill missing population values\n",
    "population_filled = population_df.with_columns([\n",
    "    pl.col(\"population\").fill_null(strategy=\"forward\")\n",
    "])\n",
    "\n",
    "# Fill with column mean for numeric columns\n",
    "numeric_cols = [col for col, dtype in covid_df.schema.items() \n",
    "               if dtype in [pl.Int64, pl.Float64]]\n",
    "covid_filled = covid_df.with_columns([\n",
    "    pl.col(col).fill_null(pl.col(col).mean()) for col in numeric_cols\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Functions**: Missing data handling\n",
    "\n",
    "---\n",
    "\n",
    "## PART 7: Creating New Columns\n",
    "\n",
    "### Exercise 7.1: Column Transformations\n",
    "\n",
    "**Business Context**: Create new metrics and calculated fields for analysis.\n",
    "\n",
    "**Tasks**:\n",
    "1. Create a 'daily_volatility' column for stocks (high - low) / close\n",
    "2. Create a 'case_fatality_rate' column for COVID data (deaths/cases)\n",
    "3. Create a 'profit_margin' column for sales data (profit/sales)\n",
    "4. Add a row count column to identify each record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use .with_columns(), arithmetic operations, .with_row_count()\n",
    "\n",
    "# Daily volatility for stocks\n",
    "stocks_metrics = stocks_df.with_columns([\n",
    "    ((pl.col(\"high\") - pl.col(\"low\")) / pl.col(\"close\")).alias(\"daily_volatility\")\n",
    "])\n",
    "\n",
    "# Case fatality rate for COVID\n",
    "covid_metrics = covid_df.with_columns([\n",
    "    (pl.col(\"new_deaths\") / pl.col(\"new_cases\")).alias(\"case_fatality_rate\")\n",
    "])\n",
    "\n",
    "# Profit margin for sales\n",
    "sales_metrics = sales_df.with_columns([\n",
    "    (pl.col(\"profit\") / pl.col(\"sales\")).alias(\"profit_margin\")\n",
    "])\n",
    "\n",
    "# Add row count\n",
    "numbered_df = sales_df.with_row_count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Functions**: Column creation and transformation\n",
    "\n",
    "---\n",
    "\n",
    "## PART 8: Reshaping Data\n",
    "\n",
    "### Exercise 8.1: Pivoting and Melting\n",
    "\n",
    "**Business Context**: Different analysis requires different data layouts.\n",
    "\n",
    "**Tasks**:\n",
    "1. Pivot COVID data to have countries as columns and dates as rows (for new_cases)\n",
    "2. Melt stock data to have 'price_type' (open, high, low, close) and 'price' columns\n",
    "3. Pivot sales data to show total sales by region and category\n",
    "4. Concatenate all stock data with company information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use .pivot(), .melt(), pl.concat()\n",
    "\n",
    "# Pivot COVID data (countries as columns)\n",
    "covid_pivot = covid_df.pivot(\n",
    "    values=\"new_cases\", \n",
    "    index=\"date\", \n",
    "    columns=\"country\"\n",
    ")\n",
    "\n",
    "# Melt stock data for price types\n",
    "stocks_melted = stocks_df.melt(\n",
    "    id_vars=[\"date\", \"symbol\", \"volume\"],\n",
    "    value_vars=[\"open\", \"high\", \"low\", \"close\"],\n",
    "    variable_name=\"price_type\",\n",
    "    value_name=\"price\"\n",
    ")\n",
    "\n",
    "# Pivot sales by region and category\n",
    "sales_pivot = sales_df.group_by([\"region\", \"category\"]).agg([\n",
    "    pl.col(\"sales\").sum()\n",
    "]).pivot(\n",
    "    values=\"sales\",\n",
    "    index=\"region\", \n",
    "    columns=\"category\"\n",
    ")\n",
    "\n",
    "# Concatenate stock and company data\n",
    "enhanced_stocks = stocks_df.join(company_df, on=\"symbol\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Functions**: Data reshaping\n",
    "\n",
    "---\n",
    "\n",
    "## PART 9: Joining Datasets\n",
    "\n",
    "### Exercise 9.1: Data Joins\n",
    "\n",
    "**Business Context**: Combine datasets to get comprehensive insights.\n",
    "\n",
    "**Tasks**:\n",
    "1. Inner join stock data with company information on 'symbol'\n",
    "2. Left join sales data with a customer information table (you'll need to create a simple one)\n",
    "3. Anti join to find stocks that don't have company information\n",
    "4. Create a summary table by joining aggregated data from multiple sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use .join() with different how parameters: \"inner\", \"left\", \"anti\"\n",
    "\n",
    "# Inner join stocks with company info\n",
    "stocks_with_info = stocks_df.join(company_df, on=\"symbol\", how=\"inner\")\n",
    "\n",
    "# Anti join to find stocks without company info\n",
    "missing_info_stocks = stocks_df.join(company_df, on=\"symbol\", how=\"anti\")\n",
    "\n",
    "# Create customer summary and join with sales\n",
    "customer_summary = sales_df.group_by(\"customer_id\").agg([\n",
    "    pl.col(\"sales\").sum().alias(\"total_spent\"),\n",
    "    pl.col(\"order_id\").count().alias(\"order_count\")\n",
    "])\n",
    "\n",
    "sales_with_customer = sales_df.join(customer_summary, on=\"customer_id\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Functions**: Various types of joins\n",
    "\n",
    "---\n",
    "\n",
    "## PART 10: Advanced Analytics\n",
    "\n",
    "### Exercise 10.1: Business Intelligence Queries\n",
    "\n",
    "**Business Context**: Answer complex business questions that combine multiple techniques.\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "1. **COVID Impact Analysis**: Find the top 5 countries with the highest peak daily cases and their total death count\n",
    "\n",
    "2. **Stock Performance Ranking**: Rank tech stocks by their return (latest price vs first price) and volatility\n",
    "\n",
    "3. **Sales Trend Analysis**: Calculate month-over-month sales growth by region\n",
    "\n",
    "4. **Market Opportunity Analysis**: Identify countries with growing populations but low technology adoption (you'll need to create a proxy metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# This requires combining multiple techniques learned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top 5 Countries by Peak Daily Cases ===\n",
      "shape: (5, 3)\n",
      "┌─────────────┬──────────────────┬──────────────┐\n",
      "│ country     ┆ peak_daily_cases ┆ total_deaths │\n",
      "│ ---         ┆ ---              ┆ ---          │\n",
      "│ str         ┆ f64              ┆ f64          │\n",
      "╞═════════════╪══════════════════╪══════════════╡\n",
      "│ Spain       ┆ 3074.0           ┆ 30127.0      │\n",
      "│ Germany     ┆ 3021.0           ┆ 29595.0      │\n",
      "│ Canada      ┆ 2827.0           ┆ 27918.0      │\n",
      "│ South Korea ┆ 2824.0           ┆ 27856.0      │\n",
      "│ France      ┆ 2727.0           ┆ 26801.0      │\n",
      "└─────────────┴──────────────────┴──────────────┘\n",
      "\n",
      "=== Peak Date Information ===\n",
      "shape: (6, 4)\n",
      "┌─────────────┬────────────┬──────────────────┬────────────┐\n",
      "│ country     ┆ date       ┆ peak_daily_cases ┆ new_deaths │\n",
      "│ ---         ┆ ---        ┆ ---              ┆ ---        │\n",
      "│ str         ┆ str        ┆ f64              ┆ f64        │\n",
      "╞═════════════╪════════════╪══════════════════╪════════════╡\n",
      "│ Germany     ┆ 2023-09-13 ┆ 3021.0           ┆ 59.0       │\n",
      "│ France      ┆ 2023-09-13 ┆ 2727.0           ┆ 56.0       │\n",
      "│ Spain       ┆ 2023-09-08 ┆ 3074.0           ┆ 60.0       │\n",
      "│ Canada      ┆ 2023-09-12 ┆ 2827.0           ┆ 56.0       │\n",
      "│ South Korea ┆ 2023-09-10 ┆ 2824.0           ┆ 55.0       │\n",
      "│ South Korea ┆ 2023-09-12 ┆ 2824.0           ┆ 58.0       │\n",
      "└─────────────┴────────────┴──────────────────┴────────────┘\n",
      "\n",
      "=== Final Summary: Top 5 Countries by Peak Daily Cases ===\n",
      "shape: (6, 4)\n",
      "┌─────────────┬──────────────────┬──────────────┬────────────┐\n",
      "│ country     ┆ peak_daily_cases ┆ total_deaths ┆ peak_date  │\n",
      "│ ---         ┆ ---              ┆ ---          ┆ ---        │\n",
      "│ str         ┆ f64              ┆ f64          ┆ str        │\n",
      "╞═════════════╪══════════════════╪══════════════╪════════════╡\n",
      "│ Spain       ┆ 3074.0           ┆ 30127.0      ┆ 2023-09-08 │\n",
      "│ Germany     ┆ 3021.0           ┆ 29595.0      ┆ 2023-09-13 │\n",
      "│ Canada      ┆ 2827.0           ┆ 27918.0      ┆ 2023-09-12 │\n",
      "│ South Korea ┆ 2824.0           ┆ 27856.0      ┆ 2023-09-10 │\n",
      "│ South Korea ┆ 2824.0           ┆ 27856.0      ┆ 2023-09-12 │\n",
      "│ France      ┆ 2727.0           ┆ 26801.0      ┆ 2023-09-13 │\n",
      "└─────────────┴──────────────────┴──────────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "# 1. **COVID Impact Analysis**: Find the top 5 countries with the highest peak daily cases and their total death count\n",
    "\n",
    "# Handle missing values\n",
    "covid_df = covid_df.fill_null(0)\n",
    "\n",
    "# Find the top 5 countries with highest peak daily cases and their total death count\n",
    "result = (\n",
    "    covid_df\n",
    "    .group_by(\"country\")\n",
    "    .agg([\n",
    "        pl.col(\"new_cases\").max().alias(\"peak_daily_cases\"),\n",
    "        pl.col(\"new_deaths\").sum().alias(\"total_deaths\")\n",
    "    ])\n",
    "    .sort(\"peak_daily_cases\", descending=True)\n",
    "    .head(5)\n",
    ")\n",
    "\n",
    "print(\"\\n=== Top 5 Countries by Peak Daily Cases ===\")\n",
    "print(result)\n",
    "\n",
    "# For a more detailed view, let's also find the date when each country had its peak\n",
    "# This approach uses a join instead of iterating through countries\n",
    "peak_info = (\n",
    "    covid_df\n",
    "    .join(\n",
    "        result.select([\"country\", \"peak_daily_cases\"]),\n",
    "        on=\"country\"\n",
    "    )\n",
    "    .filter(pl.col(\"new_cases\") == pl.col(\"peak_daily_cases\"))\n",
    "    .select([\"country\", \"date\", \"peak_daily_cases\", \"new_deaths\"])\n",
    ")\n",
    "\n",
    "print(\"\\n=== Peak Date Information ===\")\n",
    "print(peak_info)\n",
    "\n",
    "# Create a comprehensive summary\n",
    "summary = (\n",
    "    result\n",
    "    .join(\n",
    "        peak_info.select([\"country\", \"date\"]).rename({\"date\": \"peak_date\"}),\n",
    "        on=\"country\"\n",
    "    )\n",
    "    .sort(\"peak_daily_cases\", descending=True)\n",
    ")\n",
    "\n",
    "print(\"\\n=== Final Summary: Top 5 Countries by Peak Daily Cases ===\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tech Stocks Ranked by Return ===\n",
      "shape: (4, 6)\n",
      "┌────────┬─────────────────────┬─────────────┬──────────────┬────────────┬────────────┐\n",
      "│ symbol ┆ company_name        ┆ first_price ┆ latest_price ┆ return_pct ┆ volatility │\n",
      "│ ---    ┆ ---                 ┆ ---         ┆ ---          ┆ ---        ┆ ---        │\n",
      "│ str    ┆ str                 ┆ f64         ┆ f64          ┆ f64        ┆ f64        │\n",
      "╞════════╪═════════════════════╪═════════════╪══════════════╪════════════╪════════════╡\n",
      "│ META   ┆ Meta Platforms Inc. ┆ 432.19      ┆ 958.46       ┆ 121.768204 ┆ 41.456389  │\n",
      "│ GOOGL  ┆ Alphabet Inc.       ┆ 120.63      ┆ 134.69       ┆ 11.655475  ┆ 17.874941  │\n",
      "│ AMZN   ┆ Amazon.com Inc.     ┆ 124.31      ┆ 100.0        ┆ -19.555949 ┆ 16.847144  │\n",
      "│ NVDA   ┆ NVIDIA Corporation  ┆ 339.55      ┆ 117.43       ┆ -65.415992 ┆ 49.689368  │\n",
      "└────────┴─────────────────────┴─────────────┴──────────────┴────────────┴────────────┘\n",
      "\n",
      "=== Tech Stocks Ranked by Volatility ===\n",
      "shape: (4, 6)\n",
      "┌────────┬─────────────────────┬─────────────┬──────────────┬────────────┬────────────┐\n",
      "│ symbol ┆ company_name        ┆ first_price ┆ latest_price ┆ return_pct ┆ volatility │\n",
      "│ ---    ┆ ---                 ┆ ---         ┆ ---          ┆ ---        ┆ ---        │\n",
      "│ str    ┆ str                 ┆ f64         ┆ f64          ┆ f64        ┆ f64        │\n",
      "╞════════╪═════════════════════╪═════════════╪══════════════╪════════════╪════════════╡\n",
      "│ NVDA   ┆ NVIDIA Corporation  ┆ 339.55      ┆ 117.43       ┆ -65.415992 ┆ 49.689368  │\n",
      "│ META   ┆ Meta Platforms Inc. ┆ 432.19      ┆ 958.46       ┆ 121.768204 ┆ 41.456389  │\n",
      "│ GOOGL  ┆ Alphabet Inc.       ┆ 120.63      ┆ 134.69       ┆ 11.655475  ┆ 17.874941  │\n",
      "│ AMZN   ┆ Amazon.com Inc.     ┆ 124.31      ┆ 100.0        ┆ -19.555949 ┆ 16.847144  │\n",
      "└────────┴─────────────────────┴─────────────┴──────────────┴────────────┴────────────┘\n",
      "\n",
      "=== Tech Stocks Ranked by Combined Score (Return vs Volatility) ===\n",
      "shape: (4, 5)\n",
      "┌────────┬─────────────────────┬────────────┬────────────┬────────────────┐\n",
      "│ symbol ┆ company_name        ┆ return_pct ┆ volatility ┆ combined_score │\n",
      "│ ---    ┆ ---                 ┆ ---        ┆ ---        ┆ ---            │\n",
      "│ str    ┆ str                 ┆ f64        ┆ f64        ┆ f64            │\n",
      "╞════════╪═════════════════════╪════════════╪════════════╪════════════════╡\n",
      "│ GOOGL  ┆ Alphabet Inc.       ┆ 11.655475  ┆ 17.874941  ┆ 0.690223       │\n",
      "│ META   ┆ Meta Platforms Inc. ┆ 121.768204 ┆ 41.456389  ┆ 0.625341       │\n",
      "│ AMZN   ┆ Amazon.com Inc.     ┆ -19.555949 ┆ 16.847144  ┆ 0.6225         │\n",
      "│ NVDA   ┆ NVIDIA Corporation  ┆ -65.415992 ┆ 49.689368  ┆ 0.0            │\n",
      "└────────┴─────────────────────┴────────────┴────────────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "#2. **Stock Performance Ranking**: Rank tech stocks by their return (latest price vs first price) and volatility\n",
    "\n",
    "# Perform the join on stockdf and companydf\n",
    "joined_data = stocks_df.join(\n",
    "    company_df,\n",
    "    on=\"symbol\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Execute the query and collect results\n",
    "df = joined_data\n",
    "\n",
    "\n",
    "# Filter for tech companies only\n",
    "tech_df = df.filter(pl.col(\"sector\") == \"Technology\")\n",
    "\n",
    "# Calculate metrics by stock symbol\n",
    "stock_metrics = (\n",
    "    tech_df\n",
    "    .group_by(\"symbol\")\n",
    "    .agg([\n",
    "        # First price (earliest date)\n",
    "        pl.col(\"close\").filter(pl.col(\"date\") == pl.col(\"date\").min()).first().alias(\"first_price\"),\n",
    "        \n",
    "        # Latest price (latest date)\n",
    "        pl.col(\"close\").filter(pl.col(\"date\") == pl.col(\"date\").max()).first().alias(\"latest_price\"),\n",
    "        \n",
    "        # Standard deviation of daily returns for volatility\n",
    "        pl.col(\"close\").std().alias(\"price_std\"),\n",
    "        \n",
    "        # Average price for relative volatility calculation\n",
    "        pl.col(\"close\").mean().alias(\"avg_price\"),\n",
    "        \n",
    "        # Company name (first occurrence)\n",
    "        pl.col(\"company_name\").first().alias(\"company_name\")\n",
    "    ])\n",
    "    # Calculate return and volatility metrics\n",
    "    .with_columns([\n",
    "        # Return: (latest_price - first_price) / first_price * 100\n",
    "        ((pl.col(\"latest_price\") - pl.col(\"first_price\")) / pl.col(\"first_price\") * 100).alias(\"return_pct\"),\n",
    "        \n",
    "        # Volatility: price_std / avg_price * 100 (coefficient of variation as percentage)\n",
    "        (pl.col(\"price_std\") / pl.col(\"avg_price\") * 100).alias(\"volatility\")\n",
    "    ])\n",
    "    # Select and order columns\n",
    "    .select([\"symbol\", \"company_name\", \"first_price\", \"latest_price\", \"return_pct\", \"volatility\"])\n",
    ")\n",
    "\n",
    "\n",
    "# Sort by return (descending)\n",
    "return_ranking = stock_metrics.sort(\"return_pct\", descending=True)\n",
    "print(\"\\n=== Tech Stocks Ranked by Return ===\")\n",
    "print(return_ranking)\n",
    "\n",
    "# Sort by volatility (descending)\n",
    "volatility_ranking = stock_metrics.sort(\"volatility\", descending=True)\n",
    "print(\"\\n=== Tech Stocks Ranked by Volatility ===\")\n",
    "print(volatility_ranking)\n",
    "\n",
    "# Create a combined score (higher return and lower volatility is better)\n",
    "# Normalize both metrics to 0-1 range and create a score\n",
    "combined_ranking = (\n",
    "    stock_metrics\n",
    "    .with_columns([\n",
    "        # Normalize return (higher is better)\n",
    "        ((pl.col(\"return_pct\") - pl.col(\"return_pct\").min()) / \n",
    "         (pl.col(\"return_pct\").max() - pl.col(\"return_pct\").min())).alias(\"return_norm\"),\n",
    "         \n",
    "        # Normalize volatility (lower is better, so invert)\n",
    "        (1 - (pl.col(\"volatility\") - pl.col(\"volatility\").min()) / \n",
    "         (pl.col(\"volatility\").max() - pl.col(\"volatility\").min())).alias(\"volatility_norm\")\n",
    "    ])\n",
    "    # Calculate combined score (equal weight to return and volatility)\n",
    "    .with_columns([\n",
    "        ((pl.col(\"return_norm\") + pl.col(\"volatility_norm\")) / 2).alias(\"combined_score\")\n",
    "    ])\n",
    "    # Sort by combined score (descending)\n",
    "    .sort(\"combined_score\", descending=True)\n",
    "    # Select relevant columns\n",
    "    .select([\"symbol\", \"company_name\", \"return_pct\", \"volatility\", \"combined_score\"])\n",
    ")\n",
    "\n",
    "print(\"\\n=== Tech Stocks Ranked by Combined Score (Return vs Volatility) ===\")\n",
    "print(combined_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Functions**: Complex queries combining multiple operations\n",
    "\n",
    "---\n",
    "\n",
    "## PART 11: Final Challenge Project\n",
    "\n",
    "### Business Case: Multi-Client Analytics Dashboard\n",
    "\n",
    "**Scenario**: You need to create a comprehensive analytics summary for all four clients.\n",
    "\n",
    "**Deliverables**:\n",
    "1. **Executive Summary Table**: Key metrics for each dataset\n",
    "2. **Trend Analysis**: Monthly trends for all time series data\n",
    "3. **Performance Rankings**: Top performers in each category\n",
    "4. **Risk Assessment**: Identify volatile stocks, countries with concerning COVID trends\n",
    "5. **Growth Opportunities**: Regions/countries with best growth potential\n",
    "\n",
    "**Requirements**:\n",
    "- Use at least 10 different Polars functions\n",
    "- Join at least 2 datasets\n",
    "- Include rolling calculations\n",
    "- Handle missing data appropriately\n",
    "- Create meaningful new columns\n",
    "- Provide business insights with each analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your comprehensive solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Function Reference Summary\n",
    "\n",
    "By completing this exercise, you will have practiced these Polars functions:\n",
    "\n",
    "**Data Loading & Inspection**:\n",
    "- `pl.read_csv()`, `.shape`, `.head()`, `.describe()`\n",
    "\n",
    "**Selecting & Subsetting**:\n",
    "- `.select()`, `pl.col()`, `.filter()`, `.sample()`, `.head()`, `.tail()`\n",
    "\n",
    "**Grouping & Aggregation**:\n",
    "- `.group_by()`, `.agg()`, `pl.sum()`, `pl.mean()`, `pl.max()`, `pl.min()`, `pl.count()`\n",
    "\n",
    "**Time Series**:\n",
    "- `.rolling_mean()`, `.rolling_max()`, `.rolling_sum()`, date operations\n",
    "\n",
    "**Window Functions**:\n",
    "- `.over()`, `.rank()`, window calculations\n",
    "\n",
    "**Missing Data**:\n",
    "- `.drop_nulls()`, `.fill_null()`, `.is_null()`\n",
    "\n",
    "**Column Operations**:\n",
    "- `.with_columns()`, `.with_row_count()`, `.rename()`, `.drop()`\n",
    "\n",
    "**Reshaping**:\n",
    "- `.pivot()`, `.melt()`, `pl.concat()`, `.sort()`\n",
    "\n",
    "**Joining**:\n",
    "- `.join()` with various modes (inner, left, outer, anti)\n",
    "\n",
    "---\n",
    "\n",
    "## Tips for Success\n",
    "\n",
    "1. **Start Simple**: Begin with basic operations before combining complex functions\n",
    "2. **Check Your Data**: Always inspect results to ensure they make business sense\n",
    "3. **Use Method Chaining**: Polars allows elegant chaining of operations\n",
    "4. **Handle Edge Cases**: Consider missing data and edge cases in your analysis\n",
    "5. **Document Your Work**: Add comments explaining your business logic\n",
    "\n",
    "Good luck with your analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ooc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
